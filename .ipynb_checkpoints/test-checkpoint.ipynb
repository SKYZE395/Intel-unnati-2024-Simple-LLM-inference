{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "core = ov.Core()\n",
    "import os\n",
    "\n",
    "# os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "# %pip install -Uq pip\n",
    "# %pip uninstall -q -y optimum optimum-intel\n",
    "# %pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "# %pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "# \"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "# \"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "# \"torch>=2.1\"\\\n",
    "# \"datasets\" \\\n",
    "# \"accelerate\"\\\n",
    "# \"gradio>=4.19\"\\\n",
    "# \"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.38.1\" \"bitsandbytes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel(R) Core(TM) i5-10300H CPU @ 2.50GHz\n",
      "GPU.0: Intel(R) UHD Graphics (iGPU)\n",
      "GPU.1: NVIDIA GeForce GTX 1650 (dGPU)\n"
     ]
    }
   ],
   "source": [
    "devices = core.available_devices\n",
    "\n",
    "for x in devices:\n",
    "    device_name = core.get_property(x,\"FULL_DEVICE_NAME\")\n",
    "    print(f\"{x}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6a135a33f24d4d8d4bef6afa55cf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU.0', 'GPU.1'), value='CPU')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices,\n",
    "    value=core.available_devices[0],\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is read using read_model() and compiled using compile_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_id = \"Intel/neural-chat-7b-v3-1\"\n",
    "model_path = \"neural-chat-7b-v3-1-ov-int4\"\n",
    "\n",
    "if not Path(model_path).exists():\n",
    "    !optimum-cli export openvino --model {model_id} --weight-format int4 {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade-strategy eager \"optimum[openvino,nncf]\" langchain-huggingface --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <finalize object at 0x2aac3634e40; dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\weakref.py\", line 591, in __call__\n",
      "    return info.func(*info.args, **(info.kwargs or {}))\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\tempfile.py\", line 829, in _cleanup\n",
      "    cls._rmtree(name)\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\tempfile.py\", line 825, in _rmtree\n",
      "    _shutil.rmtree(name, onerror=onerror)\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\shutil.py\", line 629, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\tempfile.py\", line 817, in onerror\n",
      "    cls._rmtree(path)\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\tempfile.py\", line 825, in _rmtree\n",
      "    _shutil.rmtree(name, onerror=onerror)\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\shutil.py\", line 610, in _rmtree_unsafe\n",
      "    onerror(os.scandir, path, sys.exc_info())\n",
      "  File \"c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\shutil.py\", line 607, in _rmtree_unsafe\n",
      "    with os.scandir(path) as scandir_it:\n",
      "NotADirectoryError: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\jarvi\\\\AppData\\\\Local\\\\Temp\\\\tmpkdpcqe_g\\\\openvino_model.bin'\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 and attention_mask is not None and attention_mask[0, 0, -1, -1] < -1:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 or self.training:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:70: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_length > 1:\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "ov_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\"device\": \"CPU\", \"ov_config\": ov_config},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step. One way to look at it is as a kind of computerized record recorder or brain scanner. The way we do that is look at a picture and just take a bunch of photographs and then we use a little computer to record any kind of stimuli or\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | ov_llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Automatic task detection to text-generation-with-past (possible synonyms are: causal-lm-with-past).\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 and attention_mask is not None and attention_mask[0, 0, -1, -1] < -1:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 or self.training:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:70: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_length > 1:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\torch\\jit\\_trace.py:1116: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 50257 / 1608224 (3.1%)\n",
      "Greatest absolute difference: 0.0010986328125 at index (1, 12, 8815) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1.5901268479858685e-05 at index (1, 12, 1) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export openvino --model gpt2 ov_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell is to format the model and export it in ov_model_dir with a 4 bit precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed-Precision assignment ----------------------   0% 0/48 • 0:00:00 • -:--:--\n",
      "Mixed-Precision assignment ----------------------   0% 0/48 • 0:00:07 • -:--:--\n",
      "Mixed-Precision assignment ----------------------   0% 0/48 • 0:00:16 • -:--:--\n",
      "Mixed-Precision assignment  ---------------------   4% 2/48 • 0:00:18 • -:--:--\n",
      "Mixed-Precision assignment  ---------------------   4% 2/48 • 0:00:18 • -:--:--\n",
      "Mixed-Precision assignment - --------------------   6% 3/48 • 0:00:18 • 0:00:10\n",
      "Mixed-Precision assignment - --------------------   8% 4/48 • 0:00:26 • 0:02:21\n",
      "Mixed-Precision assignment - --------------------   8% 4/48 • 0:00:26 • 0:02:21\n",
      "Mixed-Precision assignment --- ------------------  15% 7/48 • 0:00:26 • 0:01:09\n",
      "Mixed-Precision assignment ----- ---------------  25% 12/48 • 0:00:26 • 0:00:31\n",
      "Mixed-Precision assignment ------ --------------  31% 15/48 • 0:00:26 • 0:00:22\n",
      "Mixed-Precision assignment ------- -------------  38% 18/48 • 0:00:26 • 0:00:17\n",
      "Mixed-Precision assignment -------- ------------  42% 20/48 • 0:00:27 • 0:00:14\n",
      "Mixed-Precision assignment --------- -----------  46% 22/48 • 0:00:27 • 0:00:12\n",
      "Mixed-Precision assignment ---------- ----------  50% 24/48 • 0:00:27 • 0:00:10\n",
      "Mixed-Precision assignment ----------- ---------  56% 27/48 • 0:00:27 • 0:00:08\n",
      "Mixed-Precision assignment ------------ --------  60% 29/48 • 0:00:27 • 0:00:07\n",
      "Mixed-Precision assignment ------------- -------  65% 31/48 • 0:00:27 • 0:00:06\n",
      "Mixed-Precision assignment --------------- -----  73% 35/48 • 0:00:27 • 0:00:04\n",
      "Mixed-Precision assignment ---------------- ----  77% 37/48 • 0:00:27 • 0:00:04\n",
      "Mixed-Precision assignment ----------------- ---  83% 40/48 • 0:00:27 • 0:00:03\n",
      "Mixed-Precision assignment ------------------ --  90% 43/48 • 0:00:28 • 0:00:02\n",
      "Mixed-Precision assignment --------------------- 100% 48/48 • 0:00:28 • 0:00:00\n",
      "Mixed-Precision assignment --------------------- 100% 48/48 • 0:00:28 • 0:00:00\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "|   Num bits (N) | % all parameters (layers)   | % ratio-defining parameters (layers)   |\n",
      "+================+=============================+========================================+\n",
      "|              8 | 47% (12 / 50)               | 22% (10 / 48)                          |\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "|              4 | 53% (38 / 50)               | 78% (38 / 48)                          |\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "\n",
      "Applying Weight Compression ---------------------   0% 0/50 • 0:00:00 • -:--:--\n",
      "Applying Weight Compression ---------------------   0% 0/50 • 0:00:00 • -:--:--\n",
      "Applying Weight Compression ---------------------   2% 1/50 • 0:00:00 • -:--:--\n",
      "Applying Weight Compression ---------------------   2% 1/50 • 0:00:00 • -:--:--\n",
      "Applying Weight Compression ---------------------   2% 1/50 • 0:00:00 • -:--:--\n",
      "Applying Weight Compression  --------------------   4% 2/50 • 0:00:00 • 0:00:23\n",
      "Applying Weight Compression  --------------------   4% 2/50 • 0:00:00 • 0:00:23\n",
      "Applying Weight Compression -- ------------------  10% 5/50 • 0:00:00 • 0:00:09\n",
      "Applying Weight Compression --- -----------------  16% 8/50 • 0:00:00 • 0:00:06\n",
      "Applying Weight Compression --- -----------------  18% 9/50 • 0:00:01 • 0:00:05\n",
      "Applying Weight Compression ---- ---------------  24% 12/50 • 0:00:01 • 0:00:05\n",
      "Applying Weight Compression ----- --------------  26% 13/50 • 0:00:01 • 0:00:05\n",
      "Applying Weight Compression ------ -------------  30% 15/50 • 0:00:01 • 0:00:04\n",
      "Applying Weight Compression ------ -------------  32% 16/50 • 0:00:01 • 0:00:04\n",
      "Applying Weight Compression ------ -------------  32% 17/50 • 0:00:01 • 0:00:04\n",
      "Applying Weight Compression -------- -----------  40% 20/50 • 0:00:02 • 0:00:04\n",
      "Applying Weight Compression -------- -----------  42% 21/50 • 0:00:02 • 0:00:03\n",
      "Applying Weight Compression --------- ----------  48% 24/50 • 0:00:02 • 0:00:03\n",
      "Applying Weight Compression ---------- ---------  52% 26/50 • 0:00:02 • 0:00:03\n",
      "Applying Weight Compression ----------- --------  56% 28/50 • 0:00:02 • 0:00:03\n",
      "Applying Weight Compression ------------ -------  60% 30/50 • 0:00:02 • 0:00:02\n",
      "Applying Weight Compression ------------- ------  66% 33/50 • 0:00:02 • 0:00:02\n",
      "Applying Weight Compression ------------- ------  66% 33/50 • 0:00:03 • 0:00:02\n",
      "Applying Weight Compression -------------- -----  74% 37/50 • 0:00:03 • 0:00:02\n",
      "Applying Weight Compression ---------------- ---  84% 42/50 • 0:00:03 • 0:00:01\n",
      "Applying Weight Compression ----------------- --  88% 44/50 • 0:00:03 • 0:00:01\n",
      "Applying Weight Compression ----------------- --  88% 44/50 • 0:00:03 • 0:00:01\n",
      "Applying Weight Compression ------------------ -  90% 45/50 • 0:00:03 • 0:00:01\n",
      "Applying Weight Compression -------------------   96% 48/50 • 0:00:03 • 0:00:01\n",
      "Applying Weight Compression -------------------   98% 49/50 • 0:00:03 • 0:00:01\n",
      "Applying Weight Compression -------------------- 100% 50/50 • 0:00:03 • 0:00:00\n",
      "Applying Weight Compression -------------------- 100% 50/50 • 0:00:03 • 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Automatic task detection to text-generation-with-past (possible synonyms are: causal-lm-with-past).\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 and attention_mask is not None and attention_mask[0, 0, -1, -1] < -1:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 or self.training:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openvino\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:70: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_length > 1:\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export openvino --model gpt2  --weight-format int4 ov_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is electroencephalography?\n",
      "\n",
      "Answer: Let's think step by step. First things first: electroencephalography, (\n"
     ]
    }
   ],
   "source": [
    "ov_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"ov_model_dir\",\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\"device\": \"CPU\", \"ov_config\": ov_config},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
    ")\n",
    "\n",
    "chain = prompt | ov_llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_config = {\n",
    "    \"KV_CACHE_PRECISION\": \"u8\",\n",
    "    \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\",\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, consider the body's position.\n",
      "\n",
      "We look at your brain's part: your amygdala, the part that regulates the body's electrical activity. Then, the muscles that support these muscles are activated, which means they relax, rather than react. Then another part is called the striatum, which acts as the \"bulwark\" against tension.\n",
      "\n",
      "If you start with a very large area of your skull called the amygdala (which only moves in areas of the skull, such as the"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "streamer = TextIteratorStreamer(\n",
    "    ov_llm.pipeline.tokenizer,\n",
    "    timeout=30.0,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "pipeline_kwargs = {\"pipeline_kwargs\": {\"streamer\": streamer, \"max_new_tokens\": 100}}\n",
    "chain = prompt | ov_llm.bind(**pipeline_kwargs)\n",
    "\n",
    "t1 = Thread(target=chain.invoke, args=({\"question\": question},))\n",
    "t1.start()\n",
    "\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
