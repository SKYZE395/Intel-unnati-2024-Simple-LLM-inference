{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an LLM chatbot using intel OpenVINO\n",
    "\n",
    "In the fast-paced realm of artificial intelligence (AI), chatbots have become essential tools for businesses, enhancing customer interactions and streamlining operations. Large Language Models (LLMs) are advanced AI systems designed to comprehend and generate human language. By employing deep learning algorithms and vast datasets, they grasp the intricacies of language, producing coherent and relevant responses.\n",
    "\n",
    "While traditional intent-based chatbots handle basic, single-touch inquiries such as order management, FAQs, and policy questions, LLM-powered chatbots excel at addressing more complex, multi-touch queries. These chatbots offer support in a conversational manner akin to human interactions, utilizing contextual memory. By harnessing the power of Language Models, chatbots are growing increasingly sophisticated, with an impressive ability to understand and respond to human language accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "\"torch>=2.1\"\\\n",
    "\"datasets\" \\\n",
    "\"accelerate\"\\\n",
    "\"gradio>=4.19\"\\\n",
    "\"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.38.1\" \"bitsandbytes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "core = ov.Core()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Intel(R) Core(TM) i5-10300H CPU @ 2.50GHz\n",
      "GPU: Intel(R) UHD Graphics (iGPU)\n"
     ]
    }
   ],
   "source": [
    "devices = core.available_devices\n",
    "\n",
    "for x in devices:\n",
    "    device_name = core.get_property(x,\"FULL_DEVICE_NAME\")\n",
    "    print(f\"{x}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a52eb5b8c544618d92ea2069d11b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU'), value='CPU')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices,\n",
    "    value=core.available_devices[0],\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#IMPORTANT BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is read using read_model() and compiled using compile_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_id = \"Intel/gpt2\"\n",
    "model_path = \"gpt2-ov-int4\"\n",
    "\n",
    "if not Path(model_path).exists():\n",
    "    !optimum-cli export openvino --model {model_id} --weight-format int8 {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade-strategy eager \"optimum[openvino,nncf]\" langchain-huggingface --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openV\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 and attention_mask is not None and attention_mask[0, 0, -1, -1] < -1:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openV\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 or self.training:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openV\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:70: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_length > 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'past_key_values', 'attention_mask', 'position_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "ov_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\"device\": \"CPU\", \"ov_config\": ov_config},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How big is an elephant\n",
      "\n",
      "Answer: Let's think step by step. Let's start with the larger.\n",
      "\n",
      "First, let's look at the top row. That is, the bottom row is the largest amount of animals to consider in terms of size.\n",
      "\n",
      "First, we look for the largest animal. In order to figure out where the elephant is, all you need to do is check this list of all animals on this page and then the page's largest size. That means that this page had about 7,000 elephants, with a weight of 814 and a length of 8.1 miles. However, only a little over 100,000 of them are real elephants.\n",
      "\n",
      "Next, let's consider the second row. This is a total of 4 elephants. Each of them weighs in at about\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | ov_llm\n",
    "\n",
    "question = \"How big is an elephant\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell is to format the model and export it in ov_model_dir with an 8 bit precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a Model Using the Optimum-CLI Tool\n",
    "[Back to Top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "ü§ó [Optimum Intel](https://huggingface.co/docs/optimum/intel/index) serves as the bridge between the ü§ó [Transformers](https://huggingface.co/docs/transformers/index) and [Diffusers](https://huggingface.co/docs/diffusers/index) libraries and OpenVINO, facilitating the acceleration of end-to-end pipelines on Intel architectures. It offers a user-friendly CLI interface for exporting models to the [OpenVINO Intermediate Representation (IR)](https://docs.openvino.ai/2024/documentation/openvino-ir-format.html) format.\n",
    "\n",
    "The command below demonstrates a basic model export using `optimum-cli`:\n",
    "\n",
    "```\n",
    "optimum-cli export openvino --model <model_id_or_path> --task <task> <out_dir>\n",
    "```\n",
    "\n",
    "In this command:\n",
    "- The `--model` argument specifies the model ID from the HuggingFace Hub or a local directory containing the model (saved using the `.save_pretrained` method).\n",
    "- The `--task` argument specifies one of the [supported tasks](https://huggingface.co/docs/optimum/exporters/task_manager) that the exported model should perform. For LLMs, this would be `text-generation-with-past`.\n",
    "- If model initialization requires remote code, the `--trust-remote-code` flag should also be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'past_key_values', 'attention_mask', 'position_ids']\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "|   Num bits (N) | % all parameters (layers)   | % ratio-defining parameters (layers)   |\n",
      "+================+=============================+========================================+\n",
      "|              8 | 100% (50 / 50)              | 100% (50 / 50)                         |\n",
      "+----------------+-----------------------------+----------------------------------------+\n",
      "\n",
      "Applying Weight Compression ---------------------   0% 0/50 ‚Ä¢ 0:00:00 ‚Ä¢ -:--:--\n",
      "Applying Weight Compression ---------------------   0% 0/50 ‚Ä¢ 0:00:02 ‚Ä¢ -:--:--\n",
      "Applying Weight Compression  --------------------   4% 2/50 ‚Ä¢ 0:00:05 ‚Ä¢ 0:01:30\n",
      "Applying Weight Compression - -------------------   6% 3/50 ‚Ä¢ 0:00:09 ‚Ä¢ 0:02:05\n",
      "Applying Weight Compression -- ------------------  10% 5/50 ‚Ä¢ 0:00:14 ‚Ä¢ 0:01:47\n",
      "Applying Weight Compression -- ------------------  12% 6/50 ‚Ä¢ 0:00:20 ‚Ä¢ 0:01:52\n",
      "Applying Weight Compression -- ------------------  14% 7/50 ‚Ä¢ 0:00:21 ‚Ä¢ 0:02:04\n",
      "Applying Weight Compression ----- --------------  28% 14/50 ‚Ä¢ 0:00:26 ‚Ä¢ 0:01:00\n",
      "Applying Weight Compression ------ -------------  30% 15/50 ‚Ä¢ 0:00:30 ‚Ä¢ 0:01:07\n",
      "Applying Weight Compression ------ -------------  32% 16/50 ‚Ä¢ 0:00:35 ‚Ä¢ 0:01:06\n",
      "Applying Weight Compression ------ -------------  34% 17/50 ‚Ä¢ 0:00:38 ‚Ä¢ 0:01:11\n",
      "Applying Weight Compression --------- ----------  46% 23/50 ‚Ä¢ 0:00:44 ‚Ä¢ 0:00:45\n",
      "Applying Weight Compression ---------- ---------  52% 26/50 ‚Ä¢ 0:00:55 ‚Ä¢ 0:00:56\n",
      "Applying Weight Compression ---------- ---------  52% 26/50 ‚Ä¢ 0:00:55 ‚Ä¢ 0:00:56\n",
      "Applying Weight Compression ------------- ------  66% 33/50 ‚Ä¢ 0:00:55 ‚Ä¢ 0:00:25\n",
      "Applying Weight Compression -------------------   96% 48/50 ‚Ä¢ 0:01:00 ‚Ä¢ 0:00:02\n",
      "Applying Weight Compression -------------------   98% 49/50 ‚Ä¢ 0:01:01 ‚Ä¢ 0:00:01\n",
      "Applying Weight Compression -------------------- 100% 50/50 ‚Ä¢ 0:01:03 ‚Ä¢ 0:00:00\n",
      "Applying Weight Compression -------------------- 100% 50/50 ‚Ä¢ 0:01:03 ‚Ä¢ 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "Automatic task detection to text-generation-with-past (possible synonyms are: causal-lm-with-past).\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openV\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 and attention_mask is not None and attention_mask[0, 0, -1, -1] < -1:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openV\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:56: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size == 1 or self.training:\n",
      "c:\\Users\\jarvi\\anaconda3\\envs\\openV\\lib\\site-packages\\optimum\\bettertransformer\\models\\attention.py:70: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_length > 1:\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export openvino --model gpt2  --weight-format int8 ov_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is christmas\n",
      "\n",
      "Answer: Let's think step by step.\n",
      "\n",
      "When I was first starting, my religion was \"magnificent\" but it was very different from my life with the church. In fact (the year 2000 was the great secular holiday, to me very well, and that included The Feast of St. John the Divine, the Christian holiday, which marked its day, by the very same days). So it was like a world-wide campaign. If we had been to look for a place, the church would have a much larger presence. I had to try a few things to get on board. This, however, was not the plan. I kept to my original plans of going to church each day, sometimes for one hour, sometimes two, always at night or on Sunday or the following day, for about fifteen of my religious\n"
     ]
    }
   ],
   "source": [
    "ov_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"ov_model_dir\",\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\"device\": \"CPU\", \"ov_config\": ov_config},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 160},\n",
    ")\n",
    "\n",
    "chain = prompt | ov_llm\n",
    "\n",
    "question = \"what is christmas\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_config = {\n",
    "    \"KV_CACHE_PRECISION\": \"u8\",\n",
    "    \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\",\n",
    "    \"PERFORMANCE_HINT\": \"LATENCY\",\n",
    "    \"NUM_STREAMS\": \"1\",\n",
    "    \"CACHE_DIR\": \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I hope. It's something for the weekend as I didn't get the chance to see Christ on Friday, so I'll look back at it.\n",
      "\n",
      "\n",
      "4. For those of you who wish that Christmas would not come, I'm really not sure. Why would you want to go out and expect it to come on Christmas day? Because it could potentially upset a couple of hundred people.\n",
      "\n",
      "\n",
      "5. Well then, let's have a drink. You can have a drink and some music"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "from transformers import TextIteratorStreamer\n",
    "\n",
    "streamer = TextIteratorStreamer(\n",
    "    ov_llm.pipeline.tokenizer,\n",
    "    timeout=30.0,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "pipeline_kwargs = {\"pipeline_kwargs\": {\"streamer\": streamer, \"max_new_tokens\": 100}}\n",
    "chain = prompt | ov_llm.bind(**pipeline_kwargs)\n",
    "\n",
    "t1 = Thread(target=chain.invoke, args=({\"question\": question},))\n",
    "t1.start()\n",
    "\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How big is an elephant\n",
      "\n",
      "Answer: Let's think step by step. Once you've figured out what the elephant is, there is no end to how big.\n",
      "\n",
      "When we consider what the elephant can handle, a typical elephant will be between 300 to 400 pounds, depending on how heavy the animal is. If the animal weighs more than 300 pounds a mile or more, we would call it an elephant, or more like 5,000 pounds. If the animal weighs a bit over a thousand pounds, another type of elephant is said to be around 100,000 pounds\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "ov_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"ov_model_dir\",\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs={\"device\": \"CPU\", \"ov_config\": ov_config},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 50},\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "question = \"How big is an elephant\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create function to handle the requests to and from the gradio application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(question):\n",
    "    from threading import Thread\n",
    "    from transformers import TextIteratorStreamer\n",
    "    import random\n",
    "    ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "    ov_llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"ov_model_dir\",\n",
    "        task=\"text-generation\",\n",
    "        backend=\"openvino\",\n",
    "        model_kwargs={\"device\": \"CPU\", \"ov_config\": ov_config},\n",
    "        pipeline_kwargs={\"max_new_tokens\": random.randint(200,500)},\n",
    "    )\n",
    "    template = \"\"\"Question: {question}\n",
    "\n",
    "    Answer: Here is what I found useful\\n\"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    chain = prompt | ov_llm\n",
    "\n",
    "    return chain.invoke({\"question\": question})\n",
    "    # streamer = TextIteratorStreamer(\n",
    "    #     ov_llm.pipeline.tokenizer,\n",
    "    #     timeout=30.0,\n",
    "    #     skip_prompt=True,\n",
    "    #     skip_special_tokens=True,\n",
    "    # )\n",
    "    # pipeline_kwargs = {\"pipeline_kwargs\": {\"streamer\": streamer, \"max_new_tokens\": 160}}\n",
    "    # chain = prompt | ov_llm.bind(**pipeline_kwargs)\n",
    "\n",
    "    # t1 = Thread(target=chain.invoke, args=({\"question\": question},))\n",
    "    # t1.start()\n",
    "\n",
    "    # s = \"\"\n",
    "    # for new_text in streamer:\n",
    "    #     s+=new_text\n",
    "    # return s\n",
    "\n",
    "\n",
    "def bot(input, history):\n",
    "    history = history or []\n",
    "    s = list(sum(history, ()))\n",
    "    s.append(input)\n",
    "    inp = ' '.join(s)\n",
    "    output = create(input)\n",
    "    history.append((input, output))\n",
    "    return history, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_css = \"\"\"\n",
    "@import url('https://fonts.googleapis.com/css2?family=Unbounded:wght@600&display=swap');\n",
    "@import url('https://fonts.googleapis.com/css2?family=Unbounded&display=swap');\n",
    "h1 {\n",
    "    font-family: Unbounded;\n",
    "    font-size: 35px;\n",
    "}\n",
    ".desc {\n",
    "    font-family: Unbounded;\n",
    "    font-size: 10px;\n",
    "}\n",
    "div {\n",
    "    font-family: Unbounded;\n",
    "    font-size: 21px;\n",
    "}\n",
    ".progress-text {\n",
    "    font-family: Unbounded;\n",
    "    font-size: 10px;\n",
    "}\n",
    "pre{\n",
    "    white-space: pre-wrap;\n",
    "    overflow-wrap: break-word;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = gr.Blocks(css=custom_css)\n",
    "prompt = \"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Chatbot\n",
    "Now, when model created, we can setup Chatbot interface using [Gradio](https://www.gradio.app/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Compiling the model to CPU ...\n",
      "Compiling the model to CPU ...\n",
      "Compiling the model to CPU ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with block:\n",
    "    gr.Markdown(\"\"\"<h1><center><img src=\"https://prosza.000webhostapp.com/assets/logo-removebg.png\" style=\"height:100px; width:100px;filter: invert(100%);\">ZEN(2) ChatBot</center></h1>\n",
    "    <div class='desc'><center>(This model is created using gpt2 and is running on the edge with INT8 precision; weight compression done using optimum-Intel/NNCF.)</center></div>\n",
    "    \"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=prompt)\n",
    "    state = gr.State()\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    submit.click(bot, inputs=[message, state], outputs=[chatbot, state])\n",
    "\n",
    "block.launch(debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
