# Intel-unnati-2024-Simple-LLM-inference
The intel unnati industrial training aims at running simple LLM inference on the edge using intel OpenVINO to optimize and quantize the deployment of the model using CPU.

IMPORTANT: Since the model was more than the size allowed to upload on github you will have to download the model on the machine that you are running this on and then convert it to the IR format using optimum-cli

To run the interface

1) Run the test.ipynb
2) Start the gradio server at the end of the notebook
3) Enter prompt and get output
